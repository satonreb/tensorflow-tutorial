{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 sociation of autonomous individuals \n",
      "1000 y working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[64:100])\n",
    "print(valid_size, valid_text[64:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic instituti'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z   i\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0), id2char(9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1: ['ons anarchist', 'n from the na', 'gnificant tha', ' drugs confus', 'of the origin', 'at least not ', 'st daily coll', 'cky ricardo t']\n",
      "train 2: ['ts advocate s', 'ational media', 'an in jersey ', 'sion inabilit', 'nal document ', ' parliament s', 'lege newspape', 'this classic ']\n",
      "valid 1: [' a']\n",
      "valid 2: ['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_unrollings = 12\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print('train 1:', batches2string(train_batches.next()))\n",
    "print('train 2:', batches2string(train_batches.next()))\n",
    "print('valid 1:', batches2string(valid_batches.next()))\n",
    "print('valid 2:', batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.]]),\n",
       " array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.]]),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          1.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "          0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.]]),\n",
       " array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.]])]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298413 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "wozirfs n akahlxtctznbxsrefaiimsqa mwolzdnnexgnd  ru fqeabu ushspdeykiliqlne sxu\n",
      "lim ebavcr q dabivtzkalixvacrpxyp grsjqeet rl isloyryneuthdausgqvoaaqqjltso lmbz\n",
      "jma ntali oourz dougimvafcaifjucvsgeied nfnycygt pbrur klvx kn kirrif osd no  nh\n",
      "gdmre crobrr   t llu omrajrcxnmipujuokly eivhlmxsnfaln ei or tfaeytnegdannbqv  d\n",
      "asv eknetrtselnfy eqa spaotptusmz  phzt wrwdltcamoixccohalab revkpsgkkjjocovs zp\n",
      "================================================================================\n",
      "Validation set perplexity: 20.45\n",
      "Average loss at step 100: 2.705587 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.16\n",
      "Validation set perplexity: 13.79\n",
      "Average loss at step 200: 2.400011 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.36\n",
      "Validation set perplexity: 10.09\n",
      "Average loss at step 300: 2.242891 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.63\n",
      "Validation set perplexity: 11.74\n",
      "Average loss at step 400: 2.014155 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 9.92\n",
      "Average loss at step 500: 1.950460 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 600: 1.906980 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 700: 1.895306 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.43\n",
      "Validation set perplexity: 8.73\n",
      "Average loss at step 800: 1.985925 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 900: 1.921023 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.45\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 1000: 1.888484 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.01\n",
      "================================================================================\n",
      "ine daaker and dusmodea jarend caiediinac parphen the esqave in two od frome nuc\n",
      "mer watly zero girm and in inicuciteanlyytyl rivhay on kably ib the shape b onar\n",
      " hiskaday iplanivejy klouke the spaked jame three and prowiin reind was and oxen\n",
      "for sisuany afaga on mass adly ba  one qur is a d onot th one zero one vero yeiv\n",
      "ith and taencen or the this in a and sh soviode fivese the deifpertk as sen b of\n",
      "================================================================================\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 1100: 1.865866 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 1200: 1.876248 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 1300: 1.795739 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 1400: 1.786943 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 1500: 1.654496 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.63\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 1600: 1.614387 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 1700: 1.702764 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1800: 1.802758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 1900: 1.811820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 2000: 1.809308 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "================================================================================\n",
      "se from ove bo in opeld vedisce amalar incedsel acconsen extinkinagiona a dioke \n",
      "puad nialect a sting aning is the lead wilieist of applialican pacralady our of \n",
      "aaliadifuined cantiad tean oll can norpenos dimsarie manieagikal contedor anal a\n",
      "nentor hasial iapheady in the all andolman analla a bollo jantanguagubs fulinist\n",
      "lyssiye als an for an an ante norsewa be langane ablial in omasore spricianted s\n",
      "================================================================================\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 2100: 1.871251 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.28\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 2200: 1.861248 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 2300: 1.834505 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 2400: 1.831750 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 2500: 1.920389 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 2600: 1.839900 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.79\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 2700: 1.761684 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 2800: 1.757462 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 2900: 1.786824 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 3000: 1.748136 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "ressing constate di state phires the langout in negets af american texm cnamies \n",
      "qua a new for or diving it one five citinin of which its was incame botuey eatio\n",
      "nitly fvle wintal its mose in georging the dizh empervelvacce othe seclors trans\n",
      "dec as the dafide frelee ly mime rulish eet gohriclec glough its to the dow of g\n",
      "d jiessis that a the plations beleng for elecza gaving greeting unish six six li\n",
      "================================================================================\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 3100: 1.749407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 3200: 1.816734 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 3300: 1.727273 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 3400: 1.836600 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 3500: 1.764008 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.01\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 3600: 1.753266 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 3700: 1.772043 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 3800: 1.700980 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 3900: 1.747930 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 4000: 1.816530 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.61\n",
      "================================================================================\n",
      "meed the in assilistipist to mucters oldu b or mody forean are diagnow other sle\n",
      "wid is nated his in assition triked parnion of imlity to ring nead extre of alli\n",
      "quezian ana cautive of crosty by cyiling liaged cytusie unial malations a adliam\n",
      "nite by bemes abeia a united in time prodice to kningsiby is the minity asticity\n",
      "ut obligivity chitnio riehe wimmed mgletne ancisiations and optoria pomes willii\n",
      "================================================================================\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 4100: 1.767833 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 4200: 1.874492 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 4300: 1.785409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 4400: 1.795428 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.71\n",
      "Average loss at step 4500: 1.775104 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 4600: 1.691801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 4700: 1.703981 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 4800: 1.731012 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 4900: 1.765953 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 5000: 1.630889 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "mowsish austbther the featent are the munnition football fies themected by persi\n",
      "four be for the conternational films the arction pares which wells the maxal of \n",
      "kerely vocal us the tuctsedpam but the guiv resiver iversective k teachlinbus it\n",
      "y naw the archill of norrearince and mierbrest agto zero zero gange the construc\n",
      "bing the aimictoly the spection team haverflath rounce number ley miching the fe\n",
      "================================================================================\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 5100: 1.765021 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 5200: 1.722872 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 5300: 1.729473 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 5400: 1.736849 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 5500: 1.694254 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 5600: 1.706630 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 5700: 1.643929 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 5800: 1.654608 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 5900: 1.650413 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 6000: 1.663512 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.86\n",
      "================================================================================\n",
      "an deserate de it of the rigiel piditimely not fine bottuls univelyry eff on the\n",
      "vement hay seven beride of or is one nine zero one one nine zero two spuranfore \n",
      "urrar beter the taion bas that foolish welss arfurise for the which deface imarc\n",
      "on naf more include clubs ussilish furn itsect quenerhactoy and mostralte ligeri\n",
      "loy constulting surol is history as lings charnualian player trua s fours instia\n",
      "================================================================================\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 6100: 1.631221 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 6200: 1.616897 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 6300: 1.683160 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 6400: 1.666515 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 6500: 1.680588 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 6600: 1.753166 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.92\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 6700: 1.709685 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 6800: 1.608472 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.04\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 6900: 1.628053 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 7000: 1.726127 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.10\n",
      "================================================================================\n",
      "ft onli helled a to turn of yin sainen staten surg the lot roversputtic to sot p\n",
      "y indime democes this for moster rtadiable ali at shories rearis quighted the re\n",
      "h joved howe of for one nine eight three gerce sho chan the iseven recame rased \n",
      "igh fransh allowed that choquitiotmaje though classsition the tilo sectralional \n",
      "ar at preatites quell as non the spargona of hon ccroossoc retict hensilian in t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.90\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  wx = tf.concat([ix,fx,cx,ox], axis=1)  # <---- added this\n",
    "  wm = tf.concat([im,fm,cm,om], axis=1)  # <---- added this\n",
    "  wb = tf.concat([ib,fb,cb,ob], axis=1)  # <---- added this\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    x = tf.matmul(i, wx) + tf.matmul(o, wm) + wb                       # <---- added this\n",
    "    xi, xf, xc, xo = tf.split(value=x, num_or_size_splits=4, axis=1)   # <---- added this\n",
    "    input_gate = tf.sigmoid(xi)\n",
    "    forget_gate = tf.sigmoid(xf)\n",
    "    update = xc\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(xo)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295797 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "qai ojftorio dbsasihbrfgpttsbrmnlds fploaniejbioztluf ogb  vueiblpjgog nhrkfkhox\n",
      "bont uyz vsbam atjy ynjlesiaygodkj ckgegilagkyigznqz ayo ih tnttvanw mjevieiukas\n",
      "i aentbffh x  mlhkyskwxiiqfnujatd lghfofusifnesi rt docertgfwit ltitizcidsitwo  \n",
      "cbfui a etexiigbl ypgiilonqa gmanwsac jhjlqvrfjzjdni lilwnxmrnslsneuomjxlomag uq\n",
      "suognehvsveg ogbd lzv ukbaqmdgtgcoef  s  atisk ntnunlfi  igkujuulqn nruapjqtbnfm\n",
      "================================================================================\n",
      "Validation set perplexity: 20.59\n",
      "Average loss at step 100: 2.696001 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.94\n",
      "Validation set perplexity: 12.00\n",
      "Average loss at step 200: 2.367294 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.47\n",
      "Validation set perplexity: 10.38\n",
      "Average loss at step 300: 2.233170 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.27\n",
      "Validation set perplexity: 9.52\n",
      "Average loss at step 400: 2.179252 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.51\n",
      "Validation set perplexity: 9.52\n",
      "Average loss at step 500: 2.117375 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 10.10\n",
      "Average loss at step 600: 2.027342 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.63\n",
      "Validation set perplexity: 9.65\n",
      "Average loss at step 700: 1.944619 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.26\n",
      "Validation set perplexity: 9.70\n",
      "Average loss at step 800: 1.909757 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 900: 1.917960 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 8.82\n",
      "Average loss at step 1000: 1.916439 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.41\n",
      "================================================================================\n",
      "menoal rodelt condrucin exueeguader to to ed wead the jaber formo aris unactecel\n",
      "terstakers one mowners of ppee of theecal boctyer impbal enbetorst had apveral d\n",
      "cy muman und dornt of womegh one nine to molim rocal joll and acwhsoned concona \n",
      "lerternets motwerned dereon bown bjerter beentisex pretortences ruchan receat in\n",
      "vent the maters roherdat of westrumen six sowers to heed of us and newseers oft \n",
      "================================================================================\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 1100: 1.889290 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 1200: 1.829261 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 1300: 1.785312 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 8.21\n",
      "Average loss at step 1400: 1.882476 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 1500: 1.835022 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.01\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 1600: 1.854279 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 1700: 1.784724 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.78\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 1800: 1.770536 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 1900: 1.736089 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 2000: 1.762310 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "alilod rath the hintwo netrers of reactian the actisi which ashatinguted seven f\n",
      "deferal oro abansinessate of subber to etrow voltmanisic dayind indelinger a sta\n",
      "ured ox zero zero zero percontivernal polound the s semoliatn atgy andoduridetd \n",
      "olan some minates for qiodoher dadan necerters liftwo claba actirem mothinor men\n",
      "bed s dusinits publishendent two cuncitzon teads somes h szauta soto while damed\n",
      "================================================================================\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 2100: 1.696917 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 2200: 1.710198 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 2300: 1.730557 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 2400: 1.688924 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 2500: 1.759197 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 2600: 1.817213 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 2700: 1.833076 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 2800: 1.713304 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 2900: 1.781409 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 3000: 1.771755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "================================================================================\n",
      "heas whan fusism seor for ecclus partion wuty consprary the partes the rises tha\n",
      "adons pissel of hers erczer the noil cearomates varual to justred d roms beconti\n",
      "usite but as priney to someticism searsi geocsetis two feaccivis of both kiez th\n",
      "is from the cencessip ecpecitio pranh seven nine three nine mink undepsain arche\n",
      "verper accimeng worls ecompent of that sound greames of is posqs the recompastie\n",
      "================================================================================\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 3100: 1.787365 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 3200: 1.731330 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 3300: 1.739849 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 3400: 1.774499 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 3500: 1.721906 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 3600: 1.675353 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 3700: 1.831789 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 3800: 1.684536 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 3900: 1.681125 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 4000: 1.662355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "f or that to at tep causof many sower is one nine three zero nine zero two zero \n",
      "w lantla theory effementive for of is spentace of two one mayve ina wamd france \n",
      " to last not buien that teud of out comeoneht becaps struble t is tris dendeinn \n",
      " ther aust warned cause brotweann the avacial logen witm to mactle was es  exper\n",
      "p for fraquence bumt yilsonach temntd phallern mand the france me ey to tap pros\n",
      "================================================================================\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 4100: 1.710704 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 4200: 1.765474 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 4300: 1.733253 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 4400: 1.692691 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.89\n",
      "Average loss at step 4500: 1.693356 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 4600: 1.732837 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 4700: 1.724926 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 4800: 1.713375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 4900: 1.709369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 5000: 1.689141 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.98\n",
      "================================================================================\n",
      "gre book in the bied himptent when satic agen were astens arease or btiter nord \n",
      "arland ten liclinedtobotes in throve fease them costern bih mate ray the entingb\n",
      "ces controctial datey view prokaten hominiant th met the from gocreas an pet the\n",
      "x such and mates trout in english the war secotedy even vace attempt atail eover\n",
      " on moptorst terds renmerst but bctreath livered frow astentt of bikes the tthe \n",
      "================================================================================\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 5100: 1.685354 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 5200: 1.625583 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 5300: 1.692136 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 5400: 1.639637 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 5500: 1.674974 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 5600: 1.714485 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 5700: 1.703175 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.79\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 5800: 1.749852 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 5900: 1.718677 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 6000: 1.734167 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.46\n",
      "================================================================================\n",
      "man king first one eighhtense and the corma the wrair argered in the cond faunge\n",
      "jorpus incaused demaskel jonade work of neurs aararist meneerials despan nael an\n",
      "wes the as liciales toightist av the seven three seven eight nine nine bintger d\n",
      "quas and conalanna boya gafkunon precen aralizational licens one nine two zero u\n",
      "zare consirent go three fival americania are islations the polite four nine they\n",
      "================================================================================\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 6100: 1.734597 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 6200: 1.811059 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 6300: 1.796105 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 6400: 1.763982 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 6500: 1.734656 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.37\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 6600: 1.688065 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 6700: 1.622537 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.59\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 6800: 1.663665 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 6900: 1.719119 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 7000: 1.689843 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "ment june joins aspeakarrs unter offeren states concurent treaded attratens boon\n",
      "p was breaziangerate germisia alar paila noldan in historical again five zero fi\n",
      "ot dealded reasides are ipt to was the exciraring blandic amind all mavous and b\n",
      "purown views if money beyohed for ubulfic twere the caller by ho male rppeeny ar\n",
      "le reporrect for vlob on the sellized which as to four their loudent of the one \n",
      "================================================================================\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 7100: 1.672100 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 7200: 1.632188 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 7300: 1.682719 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 7400: 1.717268 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 7500: 1.620825 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 7600: 1.650337 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.97\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 7700: 1.664067 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 7800: 1.604047 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 7900: 1.599688 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 8000: 1.602218 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "================================================================================\n",
      "zer british law at the pluce its or ronction becollarient demight of the susmin \n",
      "quality was to dhreader ducy prosusm prime ure causts five at claceration dore t\n",
      "led indivious onored heat stride that be the as the christing had charled orsang\n",
      "once pincessaly terms playists in evens anno form colarrical ono nine when mush \n",
      "pion with different riffa on noma hatmanefice sco kean the furffictional buts te\n",
      "================================================================================\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 8100: 1.633228 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 8200: 1.679355 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 8300: 1.661613 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 8400: 1.721325 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 8500: 1.593817 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 8600: 1.620031 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.74\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 8700: 1.671693 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 8800: 1.691301 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.00\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 8900: 1.758101 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 9000: 1.649894 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.39\n",
      "================================================================================\n",
      "uts musile deverers to mof the the metaly idetem of the first heligians office c\n",
      "with the leapposite another emethess in proted way the eculled well in it from u\n",
      "nom alwas of one seven four zero seven monavies d murdonue for beeneodhat around\n",
      "iligaties internest it stem disurctions stridel to humas groad rex great convern\n",
      "deer of the war agrited and six lasses law while monting be was actembeed not fi\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.82\n",
      "Average loss at step 9100: 1.644340 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 9200: 1.646034 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 9300: 1.610363 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 9400: 1.715570 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 9500: 1.663339 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 9600: 1.581935 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 9700: 1.619453 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 9800: 1.638151 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 9900: 1.620210 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 10000: 1.577179 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "================================================================================\n",
      "ptant soniences oncel was modquratiout from conspition the whos gloudit suboldn \n",
      "gable eximentionalist distation eargh s oxs toted the essintoble were of the bom\n",
      "x the the realagies of the agallable in some commary soldums iorst of the betwhe\n",
      "pher any tontar eouch gefory uncle represidences i was oxtemted and people ined \n",
      "were roctand and and sea intence blecolvent copert the sacial one one nine five \n",
      "================================================================================\n",
      "Validation set perplexity: 4.84\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector. # <--- This line\n",
    "\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Emebeding: \n",
    "  vocab_embedding = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], -0.1, 0.1)) # <--- This line\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    embedding = tf.nn.embedding_lookup(vocab_embedding, tf.argmax(i, dimension=1)) # <--- This line\n",
    "    output, state = lstm_cell(embedding, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocab_embedding, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.356768 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.70\n",
      "================================================================================\n",
      "ueuebeseseseseeezelelebehefeeexefewereoevedeiere e ezepeyetewevemelegeueu zeleee\n",
      "geee ejeleieuevege ele eqefereeeueaeaesewejevezeneeeyeeebeqeeeeejemeneeekesede e\n",
      "reeeieaebee selerezeseieeesepeseqeserece exeweqebeleeeo pekeieseeele ecez  eeexe\n",
      "oe efeaebe eeehegeheeelevemeqeieceieaeleteleceiebegejeaekefeie euese efeeejezexe\n",
      "mehele eyenekeeare eeefewezeaebelede eo  ehedededeeelepexexemeiegewewemeyekeaede\n",
      "================================================================================\n",
      "Validation set perplexity: 579.33\n",
      "Average loss at step 100: 2.860229 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.41\n",
      "Validation set perplexity: 16.32\n",
      "Average loss at step 200: 2.390206 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.57\n",
      "Validation set perplexity: 10.81\n",
      "Average loss at step 300: 2.268013 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.04\n",
      "Validation set perplexity: 9.97\n",
      "Average loss at step 400: 2.246893 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.73\n",
      "Validation set perplexity: 10.01\n",
      "Average loss at step 500: 2.206846 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 10.26\n",
      "Average loss at step 600: 2.196631 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 10.48\n",
      "Average loss at step 700: 2.153620 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.35\n",
      "Validation set perplexity: 10.12\n",
      "Average loss at step 800: 2.071195 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 9.21\n",
      "Average loss at step 900: 2.058846 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 9.56\n",
      "Average loss at step 1000: 1.945894 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.18\n",
      "================================================================================\n",
      "zerr a elec hovuc hweah wis aut curlasy and as paloy thas a psims the ont touper\n",
      "seriksvised nugbluchs thuce seghterivica a weallionely pde one one one eight qui\n",
      "gil camprition zero eight sashe sta spliezzs is count toloriant gholimer allials\n",
      "vist sighaarhcivician pollivia cins ca lus a tovion send de capordiing withal ra\n",
      "ciy one erob one seven elinion anse aqud sits twiek six ist bio anscity pring st\n",
      "================================================================================\n",
      "Validation set perplexity: 8.99\n",
      "Average loss at step 1100: 1.977392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 9.02\n",
      "Average loss at step 1200: 2.016375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 8.81\n",
      "Average loss at step 1300: 1.925961 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 9.10\n",
      "Average loss at step 1400: 1.880608 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.17\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 1500: 1.922036 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 8.99\n",
      "Average loss at step 1600: 1.906101 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 9.60\n",
      "Average loss at step 1700: 1.972041 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 1800: 1.981675 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 8.94\n",
      "Average loss at step 1900: 1.932153 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 2000: 1.912350 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.24\n",
      "================================================================================\n",
      "ysia bey eaguces dies nine nine of down benoweentand four sinnshouning pelish of\n",
      "tedras goverks oren orging the winfe tellign artein year bicand in ones dsinnsti\n",
      "zils thisvant a in redhave far led dprompling sentple aenoger esterive colaker a\n",
      "vave faction copplary are notelssely bite smaberment durfica vingy des and to he\n",
      "zencentiocoign two wits posisheg paltwican three forss nonul genenters newn smel\n",
      "================================================================================\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 2100: 1.865707 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.58\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 2200: 1.932946 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 2300: 1.912653 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 2400: 1.870389 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 8.59\n",
      "Average loss at step 2500: 1.978434 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 2600: 1.930512 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 2700: 1.914910 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 2800: 1.830171 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 2900: 1.817757 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 3000: 1.804628 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "================================================================================\n",
      "a borishs for inite hay tranes onist of two kuility dowerantly rea hinh toccuams\n",
      "rreedangrier by soccredated soma those of comanronce soccrotornal of baisid one \n",
      "foree ireconcouse pinuany to the by of ishistist engmitied reled an indus dounit\n",
      "alsabe conpland oniston detie renders and futroran uneforicanience scherive that\n",
      "iked worman bakectoria onlaee to init it s citionsaso and in horish and is may p\n",
      "================================================================================\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 3100: 1.825873 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 3200: 1.835582 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 3300: 1.869540 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 8.31\n",
      "Average loss at step 3400: 1.846417 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 3500: 1.902645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 3600: 1.886507 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 3700: 1.895056 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 3800: 1.913551 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 3900: 1.830521 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.11\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 4000: 1.893621 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "jeer mpvile comppsplead hadvans only three mades srive rustersh a of harme ab ch\n",
      "zadge one throtems tsp one seven sour be two and se frespsfrome canmd and coumma\n",
      "wineors a trana one six three jopish this sidericiar brangeratz one two opprecip\n",
      "bnewose paut dger deted pish alled ceomapeis one one zero two nine juns and with\n",
      "ed ffor drelreognetwapmes radma terame a three five zero two renech a densich ra\n",
      "================================================================================\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 4100: 1.914756 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 4200: 1.810309 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.83\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 4300: 1.890849 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 4400: 1.855877 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 7.90\n",
      "Average loss at step 4500: 1.839464 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 4600: 1.786330 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 4700: 1.840564 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 4800: 1.882769 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 4900: 1.835772 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 5000: 1.851455 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.36\n",
      "================================================================================\n",
      "clitic is one nine nine one eight one chortion tradionally danchelly a pacted am\n",
      "quel that hicationally the a tethadoaft open eration anothe hatatiogh he five th\n",
      "ur they appeare have but howmen virus of i for the prodets ptho hose controus fo\n",
      "xing one rechoturd one nine dition s of as howning the withed some ther filentin\n",
      "fuieestations infiction for gruchil becia thiyters ther intited opence sixua foe\n",
      "================================================================================\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 5100: 1.729961 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 5200: 1.741481 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 5300: 1.741252 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 5400: 1.603701 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 5500: 1.614483 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.42\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 5600: 1.742791 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 5700: 1.706020 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 5800: 1.719510 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 5900: 1.791983 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.38\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 6000: 1.753044 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "orioneus aled those inhodels to the natations into of holow and ad this wwo zero\n",
      "quele not virused is accen teer althocht aughn included it insive a the to five \n",
      "y pucch allord their complaistria tas a works of viruses it the as the sigminger\n",
      "zation cacoations dese while licented partiwn itparch kie of five yues the an co\n",
      "queffertician streplicuiloitigl open in look c scormal to digitaturalisuenation \n",
      "================================================================================\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 6100: 1.713586 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 6200: 1.684828 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 6300: 1.722953 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 6400: 1.745149 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 6500: 1.785653 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 6600: 1.770041 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.15\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 6700: 1.719843 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 6800: 1.697600 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 6900: 1.678369 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 7000: 1.763765 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.58\n",
      "================================================================================\n",
      "soptle of casso agankitinb efusmed was in to the fell sul stater the one one one\n",
      "bortion a guagits and filonal paybectime ybe decend givil a to knews becitional \n",
      "kive all soun casugge aristmon at a sulce their after fativaly being s other rea\n",
      "he contilusing or vationg what are isf origins such origits sween two arvarden a\n",
      "erouent of olf elect someage firms alaged hismany to of his in there the ratind \n",
      "================================================================================\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 7100: 1.724794 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 7200: 1.723257 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 7300: 1.750927 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 7400: 1.753438 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 7500: 1.824693 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.00\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 7600: 1.723085 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 7700: 1.758135 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 7800: 1.729784 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 7900: 1.702405 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.11\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 8000: 1.732105 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.52\n",
      "================================================================================\n",
      "zeran quereded any m memanicia fircordy badser world usm in ide acitueport goal \n",
      "fd na to be taggin ist star sterniss luths bribblw what limit fetchs moded filrs\n",
      "gate ispultic one nine three synine loter using is or it te topposer an experati\n",
      "kon stactiume the occulan both than to bellooy voor deatmo indidet this comploro\n",
      "vicy is in one five smardine phatiming not too typencisco copect some be litk ar\n",
      "================================================================================\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 8100: 1.733607 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 8200: 1.693596 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 8300: 1.701885 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 8400: 1.668171 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 8500: 1.644833 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 8600: 1.654075 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 8700: 1.635737 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 8800: 1.631510 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.88\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 8900: 1.657096 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 9000: 1.725309 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.79\n",
      "================================================================================\n",
      "x the reast four use when s crommating one amollow which astravition ihreto indu\n",
      "z the essarics humectt mrink piale ampristed feath supelrial qud virtion a died \n",
      "z s mevert the mod adoste cause overpceny he yodera their intent sases armisal s\n",
      "ut them quilless to const it the taked uself end one nine nine by hex humpents s\n",
      "ip one nine eight zero zero sevor is a bekized and the erstil earo which s of mp\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.70\n",
      "Average loss at step 9100: 1.693964 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 9200: 1.723659 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 9300: 1.678916 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 9400: 1.662441 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 9500: 1.674979 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 9600: 1.622947 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 9700: 1.748362 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 9800: 1.669339 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 9900: 1.665525 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 10000: 1.650584 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.74\n",
      "================================================================================\n",
      "que historyed luthjdilion machmicing langer muser mumars tour the ame arigral in\n",
      "beries and of smyssil extruations vboorted vaum as at the seshoween other sommed\n",
      "wanes the eight firge dudes lub uniuman leve mide zoottan authicationy peonaled \n",
      "ussessia tarted amputn theiry seven of f life morarleasity novell meiral cluck l\n",
      "her took were hards suppeems the electliply faxeveral line but rundish propenazi\n",
      "================================================================================\n",
      "Validation set perplexity: 5.66\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grams_set = set(train_text[i:i+2] for i in range(0, len(train_text), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 53 189\n",
      "   cp ie xg\n"
     ]
    }
   ],
   "source": [
    "bi_grams_list = sorted(list(bi_grams_set))\n",
    "bi_grams_vocabulary_size = len(bi_grams_list)\n",
    "\n",
    "\n",
    "def bi_gram2id(bi_grams_list, bi_gram):\n",
    "    if bi_gram in bi_grams_list:\n",
    "      return bi_grams_list.index(bi_gram)+1\n",
    "    else:\n",
    "        return 0 \n",
    "  \n",
    "def id2bi_gram(bi_grams_list, dictid):\n",
    "  if dictid > 0:\n",
    "    return bi_grams_list[dictid]\n",
    "  else:\n",
    "    return '  '\n",
    "\n",
    "print(bi_gram2id(bi_grams_list,' a'), bi_gram2id(bi_grams_list,'sS'), bi_gram2id(bi_grams_list,'az'), bi_gram2id(bi_grams_list,'g '))\n",
    "print(id2bi_gram(bi_grams_list,0), id2bi_gram(bi_grams_list, 96), id2bi_gram(bi_grams_list,247), id2bi_gram(bi_grams_list, 654))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "batches2string() missing 1 required positional argument: 'bi_grams_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-73b1bedd01c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mvalid_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiGramBatchGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi_grams_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train 1:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches2string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi_grams_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train 2:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches2string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi_grams_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid 1:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches2string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi_grams_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: batches2string() missing 1 required positional argument: 'bi_grams_list'"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_unrollings = 12\n",
    "\n",
    "class BiGramBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings, bi_grams_list):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    self._bi_grams_list = bi_grams_list\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, bi_grams_vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, bi_gram2id(self._bi_grams_list, self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities, bi_grams_list):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2bi_gram(bi_grams_list, c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches, bi_grams_list):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b, bi_grams_list))]\n",
    "  return s\n",
    "\n",
    "train_batches = BiGramBatchGenerator(train_text, batch_size, num_unrollings, bi_grams_list)\n",
    "valid_batches = BiGramBatchGenerator(valid_text, 1, 1, bi_grams_list)\n",
    "\n",
    "print('train 1:', batches2string(train_batches.next()), bi_grams_list)\n",
    "print('train 2:', batches2string(train_batches.next()), bi_grams_list)\n",
    "print('valid 1:', batches2string(valid_batches.next()), bi_grams_list)\n",
    "print('valid 2:', batches2string(valid_batches.next()), bi_grams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 728)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_batches.next()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
